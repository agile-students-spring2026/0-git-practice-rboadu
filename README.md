# Git Practice

[Privacy in the Age of Generative AI](https://softwareengineeringdaily.com/2023/12/18/privacy-in-the-age-of-generative-ai/)

One thing that I found interesting in "Privacy in the Age of Generative AI" is that it highlights that there are limitations to the large language models (LLM) that are used in AI. The article mentions that once the LLM learns or takes in new data, it is hard for it to unlearn certain data once it has been taken in. Unfortunately, LLMs does not have the capability to delete any data which interferes with the European Union (EU) laws of transferring sensitive data to the US. Therefore, this has caused Meta to be fined with $1.3 billion due to the breaching of this rule. This means that if an AI model is trained and receives any private data, it can influence responses after the data is provided which can cause ethical and federal issues in the future. 

### Comment from Selma

What I found interesting about this article is how businesses try to solve AI privacy by limiting access using fake data or running private models. These solutions sound helpful on the surface but they do not fully fix the deeper problems with how AI handles sensitive information. Privacy in AI is more complicated than just controlling what data is used. 